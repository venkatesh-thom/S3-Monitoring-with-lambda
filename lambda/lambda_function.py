import json
import boto3
import os
import logging
import time
from urllib.parse import unquote_plus
from io import BytesIO
from PIL import Image
import uuid
from datetime import datetime

# Configure structured logging
logger = logging.getLogger()
logger.setLevel(os.environ.get('LOG_LEVEL', 'INFO'))

# AWS clients
s3_client = boto3.client('s3')
cloudwatch = boto3.client('cloudwatch')

# Supported formats
SUPPORTED_FORMATS = ['JPEG', 'PNG', 'WEBP', 'BMP', 'TIFF']
DEFAULT_QUALITY = 85
MAX_DIMENSION = 4096

def lambda_handler(event, context):
    """
    Lambda function to process images uploaded to S3.
    Supports compression and format conversion.
    """
    start_time = time.time()
    request_id = context.aws_request_id if context else 'local'
    
    try:
        logger.info(f"REQUEST_ID: {request_id} - Received event: {json.dumps(event)}")
        
        processed_count = 0
        
        # Get the S3 event details
        for record in event['Records']:
            bucket = record['s3']['bucket']['name']
            key = unquote_plus(record['s3']['object']['key'])
            file_size = record['s3']['object'].get('size', 0)
            
            logger.info(f"REQUEST_ID: {request_id} - Processing image: {key} from bucket: {bucket}, image_size: {file_size} bytes")
            
            # Log large image warning
            if file_size > 10 * 1024 * 1024:  # 10 MB
                logger.warning(f"REQUEST_ID: {request_id} - Large image detected: {key} ({file_size} bytes)")
            
            # Download the image from S3
            download_start = time.time()
            response = s3_client.get_object(Bucket=bucket, Key=key)
            image_data = response['Body'].read()
            download_time = (time.time() - download_start) * 1000
            
            logger.info(f"REQUEST_ID: {request_id} - Downloaded in {download_time:.2f}ms")
            
            # Process the image
            process_start = time.time()
            processed_images = process_image(image_data, key, request_id)
            process_time = (time.time() - process_start) * 1000
            
            logger.info(f"REQUEST_ID: {request_id} - processing_time: {process_time:.2f}ms")
            
            # Upload processed images to the processed bucket
            processed_bucket = os.environ['PROCESSED_BUCKET']
            
            upload_start = time.time()
            for processed_image in processed_images:
                output_key = processed_image['key']
                output_data = processed_image['data']
                content_type = processed_image['content_type']
                
                s3_client.put_object(
                    Bucket=processed_bucket,
                    Key=output_key,
                    Body=output_data,
                    ContentType=content_type,
                    Metadata={
                        'original-key': key,
                        'processed-by': 'lambda-image-processor',
                        'request-id': request_id,
                        'processing-time-ms': str(int(process_time))
                    }
                )
            
            upload_time = (time.time() - upload_start) * 1000
            logger.info(f"REQUEST_ID: {request_id} - Uploaded {len(processed_images)} variants in {upload_time:.2f}ms")
            
            processed_count += len(processed_images)
            logger.info(f"REQUEST_ID: {request_id} - Successfully processed {len(processed_images)} variants of {key}")
        
        # Calculate total execution time
        total_time = (time.time() - start_time) * 1000
        
        # Publish custom metrics to CloudWatch
        publish_metrics(
            function_name=context.function_name if context else 'local',
            processing_time=total_time,
            image_count=len(event['Records']),
            success=True
        )
        
        logger.info(f"REQUEST_ID: {request_id} - COMPLETED - Total time: {total_time:.2f}ms, Processed {processed_count} images")
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Image processed successfully',
                'processed_images': processed_count,
                'execution_time_ms': int(total_time),
                'request_id': request_id
            })
        }
        
    except Exception as e:
        error_time = (time.time() - start_time) * 1000
        logger.error(f"REQUEST_ID: {request_id} - ERROR processing image after {error_time:.2f}ms: {str(e)}", exc_info=True)
        
        # Publish failure metrics
        if context:
            publish_metrics(
                function_name=context.function_name,
                processing_time=error_time,
                image_count=len(event.get('Records', [])),
                success=False
            )
        
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': str(e),
                'request_id': request_id
            })
        }


def process_image(image_data, original_key, request_id='unknown'):
    """
    Process the image: create compressed versions and convert formats.
    
    Args:
        image_data: Raw image data
        original_key: Original S3 key
        request_id: Request ID for logging
        
    Returns:
        List of processed image dictionaries
    """
    processed_images = []
    
    try:
        # Open the image
        image = Image.open(BytesIO(image_data))
        
        # Convert RGBA to RGB for JPEG compatibility
        if image.mode in ('RGBA', 'LA', 'P'):
            background = Image.new('RGB', image.size, (255, 255, 255))
            if image.mode == 'P':
                image = image.convert('RGBA')
            background.paste(image, mask=image.split()[-1] if image.mode in ('RGBA', 'LA') else None)
            image = background
        elif image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Get original format and dimensions
        original_format = image.format or 'JPEG'
        width, height = image.size
        
        logger.info(f"REQUEST_ID: {request_id} - Original image: {width}x{height}, format: {original_format}")
        
        # Resize if image is too large
        if width > MAX_DIMENSION or height > MAX_DIMENSION:
            ratio = min(MAX_DIMENSION / width, MAX_DIMENSION / height)
            new_width = int(width * ratio)
            new_height = int(height * ratio)
            image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            logger.info(f"REQUEST_ID: {request_id} - Resized to: {new_width}x{new_height}")
        
        # Generate base filename
        base_name = os.path.splitext(original_key)[0]
        unique_id = str(uuid.uuid4())[:8]
        
        # Create multiple variants
        variants = [
            {'format': 'JPEG', 'quality': 85, 'suffix': 'compressed'},
            {'format': 'JPEG', 'quality': 60, 'suffix': 'low'},
            {'format': 'WEBP', 'quality': 85, 'suffix': 'webp'},
            {'format': 'PNG', 'quality': None, 'suffix': 'png'}
        ]
        
        for variant in variants:
            output = BytesIO()
            save_format = variant['format']
            
            if variant['quality']:
                image.save(output, format=save_format, quality=variant['quality'], optimize=True)
            else:
                image.save(output, format=save_format, optimize=True)
            
            output.seek(0)
            
            # Generate output key
            extension = save_format.lower()
            if extension == 'jpeg':
                extension = 'jpg'
            
            output_key = f"{base_name}_{variant['suffix']}_{unique_id}.{extension}"
            
            # Determine content type
            content_type_map = {
                'JPEG': 'image/jpeg',
                'PNG': 'image/png',
                'WEBP': 'image/webp'
            }
            content_type = content_type_map.get(save_format, 'image/jpeg')
            
            processed_images.append({
                'key': output_key,
                'data': output.getvalue(),
                'content_type': content_type,
                'format': save_format,
                'quality': variant['quality']
            })
            
            logger.info(f"REQUEST_ID: {request_id} - Created variant: {output_key} ({save_format}, quality: {variant['quality']})")
        
        # Create thumbnail
        thumbnail = image.copy()
        thumbnail.thumbnail((300, 300), Image.Resampling.LANCZOS)
        thumb_output = BytesIO()
        thumbnail.save(thumb_output, format='JPEG', quality=80, optimize=True)
        thumb_output.seek(0)
        
        processed_images.append({
            'key': f"{base_name}_thumbnail_{unique_id}.jpg",
            'data': thumb_output.getvalue(),
            'content_type': 'image/jpeg',
            'format': 'JPEG',
            'quality': 80
        })
        
        logger.info(f"REQUEST_ID: {request_id} - Created thumbnail: {base_name}_thumbnail_{unique_id}.jpg")
        
        return processed_images
        
    except Exception as e:
        logger.error(f"REQUEST_ID: {request_id} - Image processing failed: {str(e)}", exc_info=True)
        raise


def publish_metrics(function_name, processing_time, image_count, success):
    """
    Publish custom metrics to CloudWatch.
    
    Args:
        function_name: Name of the Lambda function
        processing_time: Processing time in milliseconds
        image_count: Number of images processed
        success: Whether processing was successful
    """
    try:
        metrics = [
            {
                'MetricName': 'ProcessingTime',
                'Value': processing_time,
                'Unit': 'Milliseconds',
                'Timestamp': datetime.utcnow()
            },
            {
                'MetricName': 'ImagesProcessed',
                'Value': image_count,
                'Unit': 'Count',
                'Timestamp': datetime.utcnow()
            },
            {
                'MetricName': 'ProcessingSuccess' if success else 'ProcessingFailure',
                'Value': 1,
                'Unit': 'Count',
                'Timestamp': datetime.utcnow()
            }
        ]
        
        for metric in metrics:
            cloudwatch.put_metric_data(
                Namespace='ImageProcessor/Lambda',
                MetricData=[{
                    'MetricName': metric['MetricName'],
                    'Value': metric['Value'],
                    'Unit': metric['Unit'],
                    'Timestamp': metric['Timestamp'],
                    'Dimensions': [
                        {
                            'Name': 'FunctionName',
                            'Value': function_name
                        }
                    ]
                }]
            )
        
        logger.debug(f"Published {len(metrics)} custom metrics to CloudWatch")
        
    except Exception as e:
        # Don't fail the function if metrics publishing fails
        logger.warning(f"Failed to publish metrics: {str(e)}")
